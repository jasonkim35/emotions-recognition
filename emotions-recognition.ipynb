{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520979be-fc8e-4665-9734-3d7fed5ec73b",
   "metadata": {},
   "source": [
    "# Emotions Recognition ai project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de3f39-527e-47a7-b430-77d1249e46ac",
   "metadata": {},
   "source": [
    "### 사용할 모델 다운로드\n",
    "#### face-detection-adas-0001 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf566cc-4636-4915-bb4f-62c2a26dcc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading face-detection-adas-0001 ||################\n",
      "\n",
      "========== Downloading c:\\BrainAI1\\emotions-recognition\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.xml\n",
      "... 100%, 304 KB, 330 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading c:\\BrainAI1\\emotions-recognition\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.bin\n",
      "... 49%, 1024 KB, 172 KB/s, 5 seconds passed\n",
      "... 99%, 2048 KB, 280 KB/s, 7 seconds passed\n",
      "... 100%, 2056 KB, 281 KB/s, 7 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! omz_downloader --name face-detection-adas-0001 --precision FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f9b14e-f004-463a-947b-42aa5378a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading emotions-recognition-retail-0003 ||################\n",
      "\n",
      "========== Downloading c:\\BrainAI1\\emotions-recognition\\intel\\emotions-recognition-retail-0003\\FP16\\emotions-recognition-retail-0003.xml\n",
      "... 100%, 54 KB, 47 KB/s, 1 seconds passed\n",
      "\n",
      "========== Downloading c:\\BrainAI1\\emotions-recognition\\intel\\emotions-recognition-retail-0003\\FP16\\emotions-recognition-retail-0003.bin\n",
      "... 21%, 1024 KB, 744 KB/s, 1 seconds passed\n",
      "... 42%, 2048 KB, 1272 KB/s, 1 seconds passed\n",
      "... 63%, 3072 KB, 1873 KB/s, 1 seconds passed\n",
      "... 84%, 4096 KB, 2222 KB/s, 1 seconds passed\n",
      "... 100%, 4848 KB, 2608 KB/s, 1 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! omz_downloader --name emotions-recognition-retail-0003 --precision FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da92ad0-3b96-4552-9b23-16120ac2cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e941d6-c27a-4038-9b0b-0479eb0ce79c",
   "metadata": {},
   "source": [
    "### INFERENCE 할 DEVICE 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bf0c10-afb0-4963-ace6-ffec7a104630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU', 'GPU']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "options=core.available_devices\n",
    "\n",
    "options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adb102-a044-409f-89d2-f726bc8a36e7",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3cbe1c7-308f-4deb-b089-235a4d1f7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer shape:  [1,3,384,672]\n",
      "Output layer shape: [1,1,200,7]\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "model = core.read_model(model='./models/face-detection-adas-0001.xml')\n",
    "emotion_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "face_input_layer = face_model.input(0)\n",
    "face_output_layer = face_model.output(0)\n",
    "\n",
    "print(\"Input layer shape: \", face_input_layer.shape)\n",
    "print(\"Output layer shape:\", face_output_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e60dc81c-f040-4bf8-82b8-d81c7934231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer shape:  [1,3,64,64]\n",
      "Output layer shape: [1,5,1,1]\n"
     ]
    }
   ],
   "source": [
    "model = core.read_model(model='./models/emotions-recognition-retail-0003.xml')\n",
    "emotion_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "emotion_input_layer = emotion_model.input(0)\n",
    "emotion_output_layer = emotion_model.output(0)\n",
    "\n",
    "print(\"Input layer shape: \", emotion_input_layer.shape)\n",
    "print(\"Output layer shape:\", emotion_output_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34d2cf-5129-4227-8bdf-d2e2d8b5c134",
   "metadata": {},
   "source": [
    "### Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c5a7d91-f0b2-4d05-b9bc-be2e02dcb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"image/emotions.jpg\")\n",
    "\n",
    "resized_frame = cv2.resize(src=frame, dsize=(672, 384)) \n",
    "transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "input_frame = np.expand_dims(transposed_frame, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e28619-4ff7-4e1e-bd8e-f07342a56aa7",
   "metadata": {},
   "source": [
    "### DrawBouningBoxes() 함수만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bdd027a2-777e-4d12-aedb-38ee396f38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(output, image, conf=0.5):\n",
    "    boxes=[]\n",
    "    canvas = image.copy()\n",
    "    h,w,_ = canvas.shape \n",
    "\n",
    "    predictions = output[0][0]            # 하위 집합 데이터 프레임\n",
    "    confidence = predictions[:,2]         # conf 값 가져오기 [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "\n",
    "    top_predictions = predictions[(confidence>conf)]         # 임계값보다 큰 conf 값을 가진 예측만 선택\n",
    "\n",
    "    for detection in top_predictions:\n",
    "        box = detection[3:7] * np.array([w, h, w, h]) # 상자 위치 결정\n",
    "        (xmin, ymin, xmax, ymax) = box.astype(\"int\")  # xmin, ymin, xmax, ymax에 상자 위치 값 지정\n",
    "\n",
    "        cv2.rectangle(canvas, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)       # 사각형 만들기\n",
    "        boxes.apped(box)\n",
    "   \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5aa7dc6-69e9-4a3d-b949-6ede42d7dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawBoundingBoxes(output, frame, conf=0.5):\n",
    "    boxes = []\n",
    "    canvas = frame.copy()\n",
    "    h,w,_ = canvas.shape \n",
    "\n",
    "    predictions = output[0][0]            # 하위 집합 데이터 프레임\n",
    "    confidence = predictions[:,2]         # conf 값 가져오기 [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "\n",
    "    top_predictions = predictions[(confidence>conf)]         # 임계값보다 큰 conf 값을 가진 예측만 선택\n",
    "\n",
    "    for detection in top_predictions:\n",
    "        box = (detection[3:7] * np.array([w, h, w, h])).astype(\"int\") # 상자 위치 결정\n",
    "        (xmin, ymin, xmax, ymax) = box   # xmin, ymin, xmax, ymax에 상자 위치 값 지정\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)       # 사각형 그리기\n",
    "        boxes.append(box)     #이미지에 박스를 그린 얼굴의 위치 저장\n",
    "   \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2371ae38-3e4b-4ab9-a259-2e590a06b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawText(output, frame, face_position):\n",
    "    # emotions 딕셔너리 생성\n",
    "    emotions = {\n",
    "        0:\"neutral\",\n",
    "        1:\"happy\",\n",
    "        2:\"sad\",\n",
    "        3:\"surprise\",\n",
    "        4:\"anger\"\n",
    "    }\n",
    "    # 딕셔너리 출력하기\n",
    "    #for key, value in emotions.items():\n",
    "    #    print(key, value, end='      ')\n",
    "    #print()\n",
    "        \n",
    "    predictions = output[0,:,0,0]              # 5개의 감정 예측값 저장\n",
    "    print(\"predictions : \" + str(predictions))\n",
    "    \n",
    "    topresult_index = np.argmax(predictions)   # 5개의 감정 예측값 중 가장 높은값의 위치 저장\n",
    "    #print(\"topresult_index : \" + str(topresult_index))\n",
    "    \n",
    "    emotion = emotions[topresult_index]        # emotions에서 topresult_index 값에 해당하는 감정 저장\n",
    "    #print(\"emotion : \" + emotion)\n",
    "    \n",
    "    cv2.putText(frame, emotion,                 # 예측한 감정값 이미지에 출력하기\n",
    "                (face_position[0],face_position[1]),    #xmin, ymin 값을 가져와 위치 설정\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                (255, 0,0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe1da1bf-f5c7-44f1-a683-0a18bacac0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_output = face_model([input_frame])[face_output_layer]\n",
    "boxes = DrawBoundingBoxes(face_output, frame, conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "234d1a59-ff72-43ab-9e93-5dc24e996706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions : [0.472979   0.00922723 0.03208937 0.12683505 0.35886937]\n",
      "predictions : [1.8001714e-04 1.5403769e-03 9.2567647e-01 2.4386555e-04 7.2359294e-02]\n",
      "predictions : [5.2245511e-03 7.2852528e-04 8.5551362e-04 9.9174678e-01 1.4446168e-03]\n",
      "predictions : [1.6682513e-01 8.2122630e-01 9.5314812e-03 1.8027073e-04 2.2369029e-03]\n",
      "predictions : [3.9358638e-04 9.7996461e-01 1.5637850e-02 2.5980277e-03 1.4059981e-03]\n",
      "predictions : [5.8812648e-04 5.6197602e-02 1.8172228e-04 9.1954255e-01 2.3489922e-02]\n",
      "predictions : [6.7086075e-05 9.9930334e-01 7.4545591e-05 4.5859793e-04 9.6366646e-05]\n",
      "predictions : [5.3750403e-04 1.4339936e-04 1.6935866e-02 4.8260146e-05 9.8233497e-01]\n",
      "predictions : [0.0036957  0.9840232  0.00687442 0.00285215 0.00255446]\n",
      "predictions : [1.0526952e-01 3.5744105e-04 7.9664819e-02 1.4745106e-04 8.1456077e-01]\n",
      "predictions : [4.2153178e-03 8.6598091e-02 1.0233697e-04 8.9234114e-01 1.6743194e-02]\n",
      "predictions : [1.6983333e-03 5.4251018e-04 3.2775700e-03 1.5957872e-05 9.9446565e-01]\n",
      "predictions : [7.5116404e-03 9.7150670e-04 2.6186453e-02 7.3288323e-04 9.6459752e-01]\n",
      "predictions : [0.04095532 0.31551933 0.01228291 0.5972602  0.03398228]\n",
      "predictions : [9.0727024e-03 1.7114503e-04 1.2362380e-02 3.9072597e-04 9.7800308e-01]\n",
      "predictions : [0.00307398 0.00499583 0.00100438 0.9820846  0.00884122]\n",
      "predictions : [0.00098126 0.09196732 0.37755996 0.0342243  0.49526715]\n",
      "predictions : [0.5303635  0.41939282 0.02257544 0.00834634 0.01932196]\n",
      "predictions : [0.19252373 0.00142315 0.58823055 0.00314612 0.21467645]\n",
      "predictions : [1.7553834e-02 8.2988257e-04 5.7746582e-03 5.6383600e-05 9.7578532e-01]\n",
      "predictions : [0.45257944 0.27471524 0.02373281 0.00209387 0.24687862]\n",
      "predictions : [5.39194269e-04 2.96690250e-05 1.26310475e-02 5.01047653e-05\n",
      " 9.86749947e-01]\n",
      "predictions : [4.3208126e-04 2.5644482e-03 4.9436557e-01 6.1237148e-04 5.0202560e-01]\n",
      "predictions : [4.6189232e-03 1.6789358e-02 6.8225193e-01 4.7055044e-04 2.9586920e-01]\n",
      "predictions : [0.00545333 0.7533378  0.04306275 0.05437869 0.14376739]\n"
     ]
    }
   ],
   "source": [
    "if boxes is not None:\n",
    "    \n",
    "    for box in boxes:          #boxes에 저장된 얼굴의 위치들을 하나씩 box에 전달\n",
    "    \n",
    "        xmin, ymin, xmax, ymax = box      #box에 저장된 좌표 저장\n",
    "        emotion_input = frame[ymin:ymax,xmin:xmax]         #이미지에서 해당 얼굴 위치를 찾아 저장\n",
    "        \n",
    "        # 감정 인식 모델을 사용하기 위해 이미지 전처리\n",
    "        # Input layer shape:  [1,3,64,64]\n",
    "        resized_image = cv2.resize(src=emotion_input, dsize=(64, 64))      #이미지 사이즈 변경  (64,64,3)\n",
    "        transposed_image = resized_image.transpose(2, 0, 1)                #shape 위치 변경    (3,64,64)\n",
    "        input_image = np.expand_dims(transposed_image, 0)                  #차원 확장 (1,3,64,64)\n",
    "\n",
    "        emotion_output = emotion_model([input_image])[emotion_output_layer]  # 감정 추론\n",
    "        DrawText(emotion_output, frame, box)   # 추론의 결과값 이미지에 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c172e9db-c416-4e78-a89e-a88ccc70826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"emotion-recognition\", frame)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7139f861-ab02-4c32-877a-4b11d927d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddBackground(frame, bg):\n",
    "\n",
    "    frame_h, frame_w = frame.shape[0], frame.shape[1]\n",
    "    new_h = 500\n",
    "    new_w = int((new_h/frame_h)*frame_w)\n",
    "    frame_resize = cv2.resize(frame, (new_w, new_h))\n",
    "\n",
    "    xmax = bg.shape[1] - 400\n",
    "    ymax = bg.shape[0] - 175\n",
    "    xmin = xmax - new_w\n",
    "    ymin = ymax - new_h\n",
    "\n",
    "    bg[ymin:ymax, xmin:xmax] = frame_resize\n",
    "\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec67ac21-7440-4f8b-82e5-e99e92d6d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = \"./image/Background2.jpg\"  #사용할 배경화면 경로\n",
    "\n",
    "bg = cv2.imread(background)\n",
    "deployment = AddBackground(frame, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6389a87c-99c7-476c-8fb4-2cf6b440f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Deployment\", deployment)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aaf19313-4126-4dbe-940a-442e3d6655a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions : [0.29041976 0.0309211  0.10452626 0.00525108 0.56888175]\n",
      "predictions : [0.2558909  0.02146551 0.11981048 0.00391119 0.5989219 ]\n",
      "predictions : [0.17208959 0.01741841 0.15480074 0.00269453 0.6529967 ]\n",
      "predictions : [0.24217407 0.03205158 0.1382364  0.00450833 0.58302957]\n",
      "predictions : [0.24145117 0.0279613  0.09974975 0.00268331 0.62815446]\n",
      "predictions : [0.17862847 0.01084372 0.09252564 0.00293697 0.71506506]\n",
      "predictions : [0.18233785 0.01571486 0.15320411 0.003042   0.6457013 ]\n",
      "predictions : [0.23657809 0.00833921 0.1131467  0.00922585 0.6327102 ]\n",
      "predictions : [0.2118876  0.00425938 0.10558891 0.0107461  0.667518  ]\n",
      "predictions : [0.24164732 0.00649723 0.0982643  0.01661449 0.63697666]\n",
      "predictions : [0.32886675 0.01196046 0.12287752 0.02186635 0.5144289 ]\n",
      "predictions : [0.3703492  0.01583659 0.12861824 0.02313884 0.46205705]\n",
      "predictions : [0.35569796 0.00909118 0.1500311  0.03014392 0.4550358 ]\n",
      "predictions : [0.2614041  0.01094993 0.1332401  0.01685794 0.5775479 ]\n",
      "predictions : [0.34699845 0.01551372 0.11308628 0.02594517 0.49845642]\n",
      "predictions : [0.3102181  0.01479693 0.104945   0.02257719 0.54746276]\n",
      "predictions : [0.33560175 0.0183975  0.11150874 0.037852   0.49664006]\n",
      "predictions : [0.31437883 0.01722032 0.14976309 0.02590821 0.49272954]\n",
      "predictions : [0.22812997 0.01149878 0.12911142 0.01525547 0.6160044 ]\n",
      "predictions : [0.28639585 0.01504758 0.13264099 0.01446017 0.5514554 ]\n",
      "predictions : [0.38003325 0.01814    0.11281253 0.03561193 0.45340225]\n",
      "predictions : [0.39640033 0.02690681 0.10205797 0.08503604 0.38959885]\n",
      "predictions : [0.2893954  0.01255562 0.12214769 0.06259719 0.51330405]\n",
      "predictions : [0.29636744 0.01708395 0.13770911 0.026443   0.52239645]\n",
      "predictions : [0.36490607 0.02636164 0.09839495 0.11836004 0.39197734]\n",
      "predictions : [0.41356236 0.02096122 0.12579529 0.043019   0.39666215]\n",
      "predictions : [0.34172043 0.0290122  0.10162567 0.08216745 0.44547424]\n",
      "predictions : [0.43922666 0.03174101 0.10120195 0.0436459  0.38418448]\n",
      "predictions : [0.4922152  0.03687615 0.09652577 0.06742514 0.30695772]\n",
      "predictions : [0.42353424 0.03950416 0.10077703 0.08675312 0.3494314 ]\n",
      "predictions : [0.40346077 0.02805686 0.09694821 0.04602768 0.4255065 ]\n",
      "predictions : [0.46964198 0.04566784 0.0959138  0.05805496 0.3307214 ]\n",
      "predictions : [0.33335525 0.01930908 0.11905053 0.02707152 0.50121367]\n",
      "predictions : [0.36791438 0.02071659 0.12958927 0.02664448 0.45513532]\n",
      "predictions : [0.3385811  0.02457919 0.11037989 0.04226116 0.48419866]\n",
      "predictions : [0.45835745 0.03131764 0.09862928 0.03947839 0.37221724]\n",
      "predictions : [0.3308248  0.01687485 0.12273945 0.01476587 0.51479506]\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0) #create a VideoCapture object with the 'first' camera (your webcam)\n",
    "background = \"./image/Background2.jpg\"\n",
    "bg = cv2.imread(background)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = camera.read()             # Capture frame by frame      \n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    resized_frame = cv2.resize(src=frame, dsize=(672, 384)) \n",
    "    transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "    input_frame = np.expand_dims(transposed_frame, 0)    \n",
    "    \n",
    "    face_output = face_model([input_frame])[face_output_layer]\n",
    "    \n",
    "    boxes = DrawBoundingBoxes(face_output, frame, conf=0.5)\n",
    "    \n",
    "    if boxes is not None:\n",
    "    \n",
    "        for box in boxes:          #boxes에 저장된 얼굴의 위치들을 하나씩 box에 전달\n",
    "    \n",
    "            xmin, ymin, xmax, ymax = box      #box에 저장된 좌표 저장\n",
    "            emotion_input = frame[ymin:ymax,xmin:xmax]         #이미지에서 해당 얼굴 위치를 찾아 저장\n",
    "        \n",
    "            # 감정 인식 모델을 사용하기 위해 이미지 전처리\n",
    "            # Input layer shape:  [1,3,64,64]\n",
    "            resized_image = cv2.resize(src=emotion_input, dsize=(64, 64))      #이미지 사이즈 변경  (64,64,3)\n",
    "            transposed_image = resized_image.transpose(2, 0, 1)                #shape 위치 변경    (3,64,64)\n",
    "            input_image = np.expand_dims(transposed_image, 0)                  #차원 확장 (1,3,64,64)\n",
    "\n",
    "            emotion_output = emotion_model([input_image])[emotion_output_layer]  # 감정 추론\n",
    "            DrawText(emotion_output, frame, box)   # 추론의 결과값 이미지에 출력하기\n",
    "    \n",
    "    deployment = AddBackground(frame, bg)\n",
    "    \n",
    "    cv2.imshow('Press Spacebar to Exit', deployment)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):  # Stop if spacebar is detected\n",
    "        break\n",
    "\n",
    "camera.release()                           # Cleanup after spacebar is detected.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e944f3-2b16-409f-8006-9d6d4597245c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07811e0-94a7-4fdd-b223-12bfcd8361a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
